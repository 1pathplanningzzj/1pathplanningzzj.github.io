---
title: "JAX vs PyTorchï¼šæ·±åº¦å­¦ä¹ æ¡†æ¶å¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—"
date: 2026-01-19
draft: false
tags: ["æ·±åº¦å­¦ä¹ ", "JAX", "PyTorch", "æœºå™¨å­¦ä¹ ", "æ¡†æ¶å¯¹æ¯”"]
categories: ["çŸ¥è¯†åº“"]
---

åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼ŒJAX å’Œ PyTorch æ˜¯ä¸¤ä¸ªå¤‡å—å…³æ³¨çš„æ¡†æ¶ã€‚PyTorch ä»¥å…¶æ˜“ç”¨æ€§å’ŒåŠ¨æ€è®¡ç®—å›¾è‘—ç§°ï¼Œè€Œ JAX åˆ™ä»¥å‡½æ•°å¼ç¼–ç¨‹å’Œé«˜æ€§èƒ½è®¡ç®—è§é•¿ã€‚æœ¬æ–‡å°†æ·±å…¥å¯¹æ¯”ä¸¤è€…çš„ç‰¹ç‚¹ã€ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯ï¼Œå¸®åŠ©ä½ åšå‡ºæ˜æ™ºçš„é€‰æ‹©ã€‚

{{< colab github="1pathplanningzzj/1pathplanningzzj.github.io/blob/main/static/notebooks/jax-vs-pytorch-examples.ipynb" text="ğŸš€ åœ¨ Colab ä¸­è¿è¡Œå®Œæ•´ä»£ç ç¤ºä¾‹" >}}

## æ ¸å¿ƒè®¾è®¡å“²å­¦

### PyTorchï¼šæ˜“ç”¨æ€§ä¼˜å…ˆ

PyTorch çš„è®¾è®¡ç†å¿µæ˜¯"Pythonic"å’Œç›´è§‚ï¼š

- **å‘½ä»¤å¼ç¼–ç¨‹**ï¼šä»£ç æ‰§è¡Œå³å®šä¹‰ï¼Œç¬¦åˆ Python ä¹ æƒ¯
- **é¢å‘å¯¹è±¡**ï¼šä½¿ç”¨ `nn.Module` ç±»æ„å»ºæ¨¡å‹
- **åŠ¨æ€è®¡ç®—å›¾**ï¼šè¿è¡Œæ—¶æ„å»ºå›¾ï¼Œä¾¿äºè°ƒè¯•
- **ç”Ÿæ€å®Œå–„**ï¼šä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹å’Œå·¥å…·åº“

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 5)

    def forward(self, x):
        return self.linear(x)

model = SimpleModel()
x = torch.randn(32, 10)
output = model(x)  # ç›´æ¥è°ƒç”¨ï¼Œç¬¦åˆç›´è§‰
```

### JAXï¼šå‡½æ•°å¼ + é«˜æ€§èƒ½

JAX çš„è®¾è®¡ç†å¿µæ˜¯å‡½æ•°å¼ç¼–ç¨‹å’Œå¯ç»„åˆå˜æ¢ï¼š

- **å‡½æ•°å¼ç¼–ç¨‹**ï¼šçº¯å‡½æ•°ï¼Œæ— å‰¯ä½œç”¨
- **å¯ç»„åˆå˜æ¢**ï¼š`grad`ã€`jit`ã€`vmap` ç­‰å˜æ¢å¯è‡ªç”±ç»„åˆ
- **NumPy å…¼å®¹**ï¼šAPI ä¸ NumPy é«˜åº¦ä¸€è‡´
- **XLA ç¼–è¯‘**ï¼šè‡ªåŠ¨ç¼–è¯‘ä¼˜åŒ–ï¼Œæ€§èƒ½æè‡´

```python
import jax
import jax.numpy as jnp

def simple_model(params, x):
    W, b = params
    return jnp.dot(x, W) + b

# è‡ªåŠ¨æ±‚æ¢¯åº¦
grad_fn = jax.grad(lambda params, x, y: jnp.mean((simple_model(params, x) - y)**2))

# JIT ç¼–è¯‘åŠ é€Ÿ
simple_model_jit = jax.jit(simple_model)

# è‡ªåŠ¨å‘é‡åŒ–
batched_model = jax.vmap(simple_model, in_axes=(None, 0))
```

## æ ¸å¿ƒç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | PyTorch | JAX |
|------|---------|-----|
| **ç¼–ç¨‹èŒƒå¼** | é¢å‘å¯¹è±¡ + å‘½ä»¤å¼ | å‡½æ•°å¼ |
| **è®¡ç®—å›¾** | åŠ¨æ€å›¾ï¼ˆEagerï¼‰ | å‡½æ•°å˜æ¢ï¼ˆå¯ JITï¼‰ |
| **è‡ªåŠ¨å¾®åˆ†** | Autogradï¼ˆåå‘æ¨¡å¼ï¼‰ | Autogradï¼ˆæ­£å‘+åå‘ï¼‰ |
| **ç¼–è¯‘ä¼˜åŒ–** | TorchScript / torch.compile | XLAï¼ˆé»˜è®¤ï¼‰ |
| **å¹¶è¡ŒåŒ–** | DataParallel / DDP | vmap / pmap |
| **éšæœºæ•°** | å…¨å±€çŠ¶æ€ | æ˜¾å¼ PRNG key |
| **è°ƒè¯•** | å®¹æ˜“ï¼ˆPython debuggerï¼‰ | è¾ƒéš¾ï¼ˆJIT åï¼‰ |
| **ç”Ÿæ€** | éå¸¸ä¸°å¯Œ | å¿«é€Ÿå¢é•¿ |

## PyTorch çš„æ ¸å¿ƒä¼˜åŠ¿

### 1. æ˜“ç”¨æ€§å’Œç›´è§‚æ€§

PyTorch çš„ API è®¾è®¡éå¸¸ç¬¦åˆ Python ä¹ æƒ¯ï¼š

```python
# æ¨¡å‹å®šä¹‰ç›´è§‚
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(256, 10)
)

# è®­ç»ƒå¾ªç¯æ¸…æ™°
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = criterion(model(batch.x), batch.y)
        loss.backward()
        optimizer.step()
```

### 2. ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ

- **torchvision**ï¼šè®¡ç®—æœºè§†è§‰ï¼ˆResNet, YOLO ç­‰ï¼‰
- **torchtext**ï¼šè‡ªç„¶è¯­è¨€å¤„ç†
- **torchaudio**ï¼šéŸ³é¢‘å¤„ç†
- **Hugging Face Transformers**ï¼šé¢„è®­ç»ƒæ¨¡å‹åº“
- **PyTorch Lightning**ï¼šé«˜çº§è®­ç»ƒæ¡†æ¶
- **timm**ï¼šå›¾åƒæ¨¡å‹åº“

### 3. åŠ¨æ€è®¡ç®—å›¾

é€‚åˆéœ€è¦åŠ¨æ€æ§åˆ¶æµçš„åœºæ™¯ï¼š

```python
def dynamic_model(x, use_dropout=True):
    x = self.layer1(x)

    # åŠ¨æ€æ§åˆ¶æµ
    if use_dropout and self.training:
        x = F.dropout(x, p=0.5)

    # åŠ¨æ€å¾ªç¯
    for i in range(x.size(0)):
        if x[i].sum() > 0:
            x[i] = self.layer2(x[i])

    return x
```

### 4. å·¥ä¸šç•Œå¹¿æ³›é‡‡ç”¨

- **éƒ¨ç½²å·¥å…·**ï¼šTorchServe, ONNX
- **ç§»åŠ¨ç«¯**ï¼šPyTorch Mobile
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šMeta, Tesla, OpenAI ç­‰å¤§è§„æ¨¡ä½¿ç”¨

## JAX çš„æ ¸å¿ƒä¼˜åŠ¿

### 1. å‡½æ•°å˜æ¢çš„å¯ç»„åˆæ€§

JAX çš„æ ¸å¿ƒæ˜¯å¯ç»„åˆçš„å‡½æ•°å˜æ¢ï¼š

```python
# grad: è‡ªåŠ¨å¾®åˆ†
grad_fn = jax.grad(loss_fn)

# jit: JIT ç¼–è¯‘
fast_fn = jax.jit(loss_fn)

# vmap: è‡ªåŠ¨å‘é‡åŒ–
batched_fn = jax.vmap(loss_fn)

# ç»„åˆä½¿ç”¨
fast_batched_grad = jax.jit(jax.vmap(jax.grad(loss_fn)))
```

### 2. é«˜æ€§èƒ½è®¡ç®—

é€šè¿‡ XLA ç¼–è¯‘å™¨å®ç°æè‡´æ€§èƒ½ï¼š

```python
@jax.jit
def matmul_chain(x, W1, W2, W3):
    # XLA ä¼šè‡ªåŠ¨èåˆæ“ä½œï¼Œä¼˜åŒ–å†…å­˜è®¿é—®
    return jnp.dot(jnp.dot(jnp.dot(x, W1), W2), W3)

# æ€§èƒ½é€šå¸¸æ¯” PyTorch å¿« 2-5x
```

**æ€§èƒ½ä¼˜åŠ¿**ï¼š
- **ç®—å­èåˆ**ï¼šè‡ªåŠ¨åˆå¹¶å¤šä¸ªæ“ä½œ
- **å†…å­˜ä¼˜åŒ–**ï¼šå‡å°‘ä¸­é—´ç»“æœå­˜å‚¨
- **å¹¶è¡Œä¼˜åŒ–**ï¼šè‡ªåŠ¨åˆ©ç”¨ç¡¬ä»¶å¹¶è¡Œæ€§

### 3. è‡ªåŠ¨å‘é‡åŒ–ï¼ˆvmapï¼‰

è½»æ¾å¤„ç†æ‰¹é‡æ•°æ®å’Œé›†æˆå­¦ä¹ ï¼š

```python
# å•æ ·æœ¬å‡½æ•°
def predict_single(params, x):
    return model(params, x)

# è‡ªåŠ¨æ‰¹å¤„ç†
predict_batch = jax.vmap(predict_single, in_axes=(None, 0))

# é›†æˆå­¦ä¹ ï¼šå¤šä¸ªæ¨¡å‹å¹¶è¡Œæ¨ç†
def ensemble_predict(all_params, x):
    # all_params: (num_models, ...)
    # è‡ªåŠ¨å¹¶è¡ŒåŒ–æ‰€æœ‰æ¨¡å‹
    predictions = jax.vmap(predict_single, in_axes=(0, None))(all_params, x)
    return jnp.mean(predictions, axis=0)
```

### 4. æ­£å‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†

é€‚åˆé«˜ç»´è¾“å‡ºã€ä½ç»´è¾“å…¥çš„åœºæ™¯ï¼š

```python
# åå‘æ¨¡å¼ï¼ˆPyTorch é»˜è®¤ï¼‰ï¼šé€‚åˆ loss (æ ‡é‡) å¯¹ params (é«˜ç»´) æ±‚å¯¼
grad_reverse = jax.grad(loss_fn)

# æ­£å‘æ¨¡å¼ï¼šé€‚åˆ output (é«˜ç»´) å¯¹ input (ä½ç»´) æ±‚å¯¼
jacobian_forward = jax.jacfwd(model_fn)

# äºŒé˜¶å¯¼æ•°
hessian = jax.hessian(loss_fn)
```

### 5. æ˜¾å¼éšæœºæ•°ç®¡ç†

é¿å…å…¨å±€çŠ¶æ€ï¼Œä¿è¯å¯å¤ç°æ€§ï¼š

```python
# PyTorchï¼šå…¨å±€éšæœºçŠ¶æ€
torch.manual_seed(42)
x = torch.randn(10)  # ä¾èµ–å…¨å±€çŠ¶æ€

# JAXï¼šæ˜¾å¼ PRNG key
key = jax.random.PRNGKey(42)
key, subkey = jax.random.split(key)
x = jax.random.normal(subkey, (10,))  # æ˜¾å¼ä¼ é€’ key
```

### 6. å¤šè®¾å¤‡å¹¶è¡Œï¼ˆpmapï¼‰

ç®€æ´çš„æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œï¼š

```python
# æ•°æ®å¹¶è¡Œï¼šè‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ª GPU/TPU
@jax.pmap
def train_step(params, batch):
    loss, grads = jax.value_and_grad(loss_fn)(params, batch)
    return loss, grads

# åœ¨ 8 ä¸ªè®¾å¤‡ä¸Šå¹¶è¡Œè®­ç»ƒ
losses, grads = train_step(params, batches)  # batches: (8, batch_size, ...)
```

## å…³é”®åŒºåˆ«

### 1. çŠ¶æ€ç®¡ç†

**PyTorch**ï¼šæ¨¡å‹æŒæœ‰çŠ¶æ€ï¼ˆå‚æ•°ã€ç¼“å†²åŒºï¼‰

```python
model = MyModel()
model.weight  # å‚æ•°å­˜å‚¨åœ¨æ¨¡å‹ä¸­
optimizer = torch.optim.Adam(model.parameters())
```

**JAX**ï¼šçº¯å‡½æ•°ï¼Œå‚æ•°å¤–éƒ¨ä¼ é€’

```python
params = init_params()
def model(params, x):  # å‚æ•°æ˜¾å¼ä¼ é€’
    return jnp.dot(x, params['weight'])

# éœ€è¦ä½¿ç”¨åº“ï¼ˆå¦‚ Flax, Haikuï¼‰ç®¡ç†çŠ¶æ€
```

### 2. è®­ç»ƒå¾ªç¯

**PyTorch**ï¼šå‘½ä»¤å¼ï¼Œé€æ­¥æ‰§è¡Œ

```python
for batch in dataloader:
    optimizer.zero_grad()
    loss = model(batch)
    loss.backward()
    optimizer.step()
```

**JAX**ï¼šå‡½æ•°å¼ï¼Œé€šå¸¸ JIT ç¼–è¯‘

```python
@jax.jit
def train_step(params, opt_state, batch):
    loss, grads = jax.value_and_grad(loss_fn)(params, batch)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

for batch in dataloader:
    params, opt_state, loss = train_step(params, opt_state, batch)
```

### 3. è°ƒè¯•ä½“éªŒ

**PyTorch**ï¼š
- âœ… å¯ä»¥ç›´æ¥ä½¿ç”¨ `print()`ã€`pdb` è°ƒè¯•
- âœ… é”™è¯¯ä¿¡æ¯æ¸…æ™°ï¼ŒæŒ‡å‘å…·ä½“ä»£ç è¡Œ
- âœ… åŠ¨æ€å›¾ï¼Œå¯ä»¥éšæ—¶æ£€æŸ¥ä¸­é—´ç»“æœ

**JAX**ï¼š
- âŒ JIT ç¼–è¯‘åéš¾ä»¥è°ƒè¯•
- âŒ é”™è¯¯ä¿¡æ¯å¯èƒ½æŒ‡å‘ XLA å†…éƒ¨
- âœ… å¯ä»¥ç”¨ `jax.disable_jit()` ä¸´æ—¶å…³é—­ JIT
- âœ… `jax.debug.print()` åœ¨ JIT ä¸­æ‰“å°

### 4. å†…å­˜ç®¡ç†

**PyTorch**ï¼š
- è‡ªåŠ¨ç®¡ç† GPU å†…å­˜
- å¯èƒ½å‡ºç°å†…å­˜ç¢ç‰‡
- `torch.cuda.empty_cache()` æ‰‹åŠ¨æ¸…ç†

**JAX**ï¼š
- é¢„åˆ†é… GPU å†…å­˜ï¼ˆé»˜è®¤ 75%ï¼‰
- æ›´é«˜æ•ˆçš„å†…å­˜ä½¿ç”¨
- é€šè¿‡ `XLA_PYTHON_CLIENT_PREALLOCATE=false` æ§åˆ¶

## é€‚ç”¨åœºæ™¯

### é€‰æ‹© PyTorch çš„åœºæ™¯

1. **å¿«é€ŸåŸå‹å¼€å‘**ï¼šéœ€è¦å¿«é€Ÿè¿­ä»£å’Œå®éªŒ
2. **å¤æ‚æ§åˆ¶æµ**ï¼šæ¨¡å‹åŒ…å«å¤§é‡åŠ¨æ€é€»è¾‘
3. **å·¥ä¸šéƒ¨ç½²**ï¼šéœ€è¦æˆç†Ÿçš„éƒ¨ç½²å·¥å…·é“¾
4. **å›¢é˜Ÿåä½œ**ï¼šå›¢é˜Ÿç†Ÿæ‚‰ PyTorch ç”Ÿæ€
5. **é¢„è®­ç»ƒæ¨¡å‹**ï¼šéœ€è¦ä½¿ç”¨å¤§é‡ç°æˆæ¨¡å‹ï¼ˆHugging Faceï¼‰
6. **è®¡ç®—æœºè§†è§‰/NLP**ï¼šæ ‡å‡†ä»»åŠ¡ï¼Œç”Ÿæ€å®Œå–„

**å…¸å‹åº”ç”¨**ï¼š
- å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼ˆGPT, LLaMAï¼‰
- è®¡ç®—æœºè§†è§‰ï¼ˆç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ï¼‰
- å¼ºåŒ–å­¦ä¹ ï¼ˆåŠ¨æ€ç¯å¢ƒäº¤äº’ï¼‰
- ç ”ç©¶åŸå‹å¼€å‘

### é€‰æ‹© JAX çš„åœºæ™¯

1. **é«˜æ€§èƒ½è®¡ç®—**ï¼šéœ€è¦æè‡´æ€§èƒ½ä¼˜åŒ–
2. **ç§‘å­¦è®¡ç®—**ï¼šç‰©ç†æ¨¡æ‹Ÿã€å¾®åˆ†æ–¹ç¨‹æ±‚è§£
3. **ç ”ç©¶åˆ›æ–°**ï¼šéœ€è¦çµæ´»çš„è‡ªåŠ¨å¾®åˆ†ï¼ˆæ­£å‘ã€åå‘ã€é«˜é˜¶ï¼‰
4. **TPU è®­ç»ƒ**ï¼šGoogle Cloud TPU ä¼˜åŒ–
5. **é›†æˆå­¦ä¹ **ï¼šéœ€è¦å¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¨¡å‹
6. **å‡½æ•°å¼ç¼–ç¨‹**ï¼šåå¥½çº¯å‡½æ•°å’Œå¯ç»„åˆæ€§

**å…¸å‹åº”ç”¨**ï¼š
- å¼ºåŒ–å­¦ä¹ ï¼ˆDeepMind ä½¿ç”¨ JAXï¼‰
- ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼‰
- è´å¶æ–¯æ¨æ–­ï¼ˆæ¦‚ç‡ç¼–ç¨‹ï¼‰
- å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒï¼ˆTPU podsï¼‰

## ç”Ÿæ€ç³»ç»Ÿå¯¹æ¯”

### PyTorch ç”Ÿæ€

**æ ¸å¿ƒåº“**ï¼š
- `torch`ï¼šæ ¸å¿ƒæ¡†æ¶
- `torchvision`ã€`torchtext`ã€`torchaudio`ï¼šé¢†åŸŸåº“
- `torch.distributed`ï¼šåˆ†å¸ƒå¼è®­ç»ƒ

**é«˜çº§æ¡†æ¶**ï¼š
- PyTorch Lightningï¼šç®€åŒ–è®­ç»ƒæµç¨‹
- Hugging Face Transformersï¼šé¢„è®­ç»ƒæ¨¡å‹
- timmï¼šå›¾åƒæ¨¡å‹åº“
- MMDetectionï¼šç›®æ ‡æ£€æµ‹å·¥å…·ç®±

**éƒ¨ç½²å·¥å…·**ï¼š
- TorchServeï¼šæ¨¡å‹æœåŠ¡
- ONNXï¼šæ¨¡å‹è½¬æ¢
- PyTorch Mobileï¼šç§»åŠ¨ç«¯éƒ¨ç½²

### JAX ç”Ÿæ€

**æ ¸å¿ƒåº“**ï¼š
- `jax`ï¼šæ ¸å¿ƒæ¡†æ¶
- `jax.numpy`ï¼šNumPy å…¼å®¹ API
- `optax`ï¼šä¼˜åŒ–å™¨åº“

**ç¥ç»ç½‘ç»œåº“**ï¼š
- **Flax**ï¼šå®˜æ–¹æ¨èï¼Œçµæ´»ä¸”é«˜æ€§èƒ½
- **Haiku**ï¼šDeepMind å¼€å‘ï¼ŒSonnet é£æ ¼
- **Equinox**ï¼šç°ä»£åŒ–è®¾è®¡ï¼ŒPyTorch é£æ ¼

**ä¸“ç”¨åº“**ï¼š
- **RLax**ï¼šå¼ºåŒ–å­¦ä¹ 
- **Chex**ï¼šæµ‹è¯•å’Œè°ƒè¯•å·¥å…·
- **Orbax**ï¼šæ£€æŸ¥ç‚¹å’Œåºåˆ—åŒ–
- **JAX-MD**ï¼šåˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ

## æ€§èƒ½å¯¹æ¯”

### è®­ç»ƒé€Ÿåº¦

å…¸å‹åœºæ™¯ï¼ˆResNet-50, ImageNetï¼‰ï¼š

| æ¡†æ¶ | GPU (V100) | TPU v3 |
|------|-----------|--------|
| PyTorch | 100% | - |
| PyTorch (torch.compile) | 120% | - |
| JAX | 130% | 200% |

**JAX ä¼˜åŠ¿åœºæ™¯**ï¼š
- å°æ‰¹é‡è®­ç»ƒï¼ˆXLA ä¼˜åŒ–æ›´æ˜æ˜¾ï¼‰
- TPU è®­ç»ƒï¼ˆåŸç”Ÿæ”¯æŒï¼‰
- å¤æ‚æ•°å­¦è¿ç®—ï¼ˆç®—å­èåˆï¼‰

**PyTorch ä¼˜åŠ¿åœºæ™¯**ï¼š
- å¤§æ‰¹é‡è®­ç»ƒï¼ˆcuDNN ä¼˜åŒ–ï¼‰
- æ ‡å‡†æ¨¡å‹ï¼ˆé«˜åº¦ä¼˜åŒ–ï¼‰
- åŠ¨æ€æ§åˆ¶æµï¼ˆæ— ç¼–è¯‘å¼€é”€ï¼‰

### å†…å­˜æ•ˆç‡

```python
# JAXï¼šæ›´é«˜æ•ˆçš„å†…å­˜ä½¿ç”¨
@jax.jit
def efficient_fn(x):
    # XLA è‡ªåŠ¨ä¼˜åŒ–å†…å­˜å¸ƒå±€
    return jnp.sum(jnp.exp(x) * jnp.log(x))

# PyTorchï¼šéœ€è¦æ‰‹åŠ¨ä¼˜åŒ–
def manual_fn(x):
    # å¯èƒ½åˆ›å»ºå¤šä¸ªä¸­é—´å¼ é‡
    return torch.sum(torch.exp(x) * torch.log(x))
```

## ä»£ç ç¤ºä¾‹ï¼šå®Œæ•´è®­ç»ƒæµç¨‹

### PyTorch ç‰ˆæœ¬

```python
import torch
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰æ¨¡å‹
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.layers(x)

# è®­ç»ƒ
model = MLP().cuda()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for epoch in range(10):
    for batch_x, batch_y in dataloader:
        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()

        optimizer.zero_grad()
        logits = model(batch_x)
        loss = criterion(logits, batch_y)
        loss.backward()
        optimizer.step()
```

### JAX ç‰ˆæœ¬ï¼ˆä½¿ç”¨ Flaxï¼‰

```python
import jax
import jax.numpy as jnp
from flax import linen as nn
import optax

# å®šä¹‰æ¨¡å‹
class MLP(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(256)(x)
        x = nn.relu(x)
        x = nn.Dense(10)(x)
        return x

# åˆå§‹åŒ–
model = MLP()
params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 784)))
optimizer = optax.adam(1e-3)
opt_state = optimizer.init(params)

# è®­ç»ƒæ­¥éª¤ï¼ˆJIT ç¼–è¯‘ï¼‰
@jax.jit
def train_step(params, opt_state, batch_x, batch_y):
    def loss_fn(params):
        logits = model.apply(params, batch_x)
        return optax.softmax_cross_entropy_with_integer_labels(logits, batch_y).mean()

    loss, grads = jax.value_and_grad(loss_fn)(params)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    for batch_x, batch_y in dataloader:
        params, opt_state, loss = train_step(params, opt_state, batch_x, batch_y)
```

## å­¦ä¹ æ›²çº¿

### PyTorch

- **å…¥é—¨**ï¼šâ­â­â­â­â­ï¼ˆéå¸¸å®¹æ˜“ï¼‰
- **è¿›é˜¶**ï¼šâ­â­â­â­ï¼ˆæ–‡æ¡£ä¸°å¯Œï¼‰
- **ç²¾é€š**ï¼šâ­â­â­ï¼ˆéœ€è¦ç†è§£åˆ†å¸ƒå¼è®­ç»ƒï¼‰

**å­¦ä¹ è·¯å¾„**ï¼š
1. åŸºç¡€ï¼šå¼ é‡æ“ä½œã€è‡ªåŠ¨å¾®åˆ†
2. æ¨¡å‹ï¼š`nn.Module`ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
3. è¿›é˜¶ï¼šè‡ªå®šä¹‰å±‚ã€åˆ†å¸ƒå¼è®­ç»ƒ
4. éƒ¨ç½²ï¼šTorchScriptã€ONNX

### JAX

- **å…¥é—¨**ï¼šâ­â­â­ï¼ˆéœ€è¦ç†è§£å‡½æ•°å¼ç¼–ç¨‹ï¼‰
- **è¿›é˜¶**ï¼šâ­â­â­â­ï¼ˆéœ€è¦ç†è§£ JITã€vmapï¼‰
- **ç²¾é€š**ï¼šâ­â­â­â­â­ï¼ˆéœ€è¦æ·±å…¥ç†è§£ XLAï¼‰

**å­¦ä¹ è·¯å¾„**ï¼š
1. åŸºç¡€ï¼šNumPy APIã€çº¯å‡½æ•°
2. å˜æ¢ï¼š`grad`ã€`jit`ã€`vmap`
3. ç¥ç»ç½‘ç»œï¼šé€‰æ‹©åº“ï¼ˆFlax/Haikuï¼‰
4. è¿›é˜¶ï¼š`pmap`ã€è‡ªå®šä¹‰æ¢¯åº¦ã€XLA ä¼˜åŒ–

## è¿ç§»æŒ‡å—

### ä» PyTorch åˆ° JAX

**ä¸»è¦å˜åŒ–**ï¼š
1. **å»é™¤ç±»**ï¼šç”¨çº¯å‡½æ•°æ›¿ä»£ `nn.Module`
2. **æ˜¾å¼å‚æ•°**ï¼šå‚æ•°ä¸å†å­˜å‚¨åœ¨æ¨¡å‹ä¸­
3. **JIT ç¼–è¯‘**ï¼šç”¨ `@jax.jit` åŠ é€Ÿ
4. **éšæœºæ•°**ï¼šä½¿ç”¨æ˜¾å¼ PRNG key

**è¿ç§»æ­¥éª¤**ï¼š
```python
# PyTorch
class Model(nn.Module):
    def __init__(self):
        self.weight = nn.Parameter(torch.randn(10, 5))

    def forward(self, x):
        return x @ self.weight

# JAX (Flax)
class Model(nn.Module):
    @nn.compact
    def __call__(self, x):
        weight = self.param('weight', nn.initializers.normal(), (10, 5))
        return x @ weight
```

### ä» JAX åˆ° PyTorch

**ä¸»è¦å˜åŒ–**ï¼š
1. **æ·»åŠ ç±»**ï¼šç”¨ `nn.Module` å°è£…
2. **éšå¼å‚æ•°**ï¼šå‚æ•°å­˜å‚¨åœ¨æ¨¡å‹ä¸­
3. **å»é™¤ JIT**ï¼šé»˜è®¤ eager æ‰§è¡Œ
4. **å…¨å±€éšæœºæ•°**ï¼šä½¿ç”¨ `torch.manual_seed()`

## æ€»ç»“ä¸å»ºè®®

### å¿«é€Ÿå†³ç­–æŒ‡å—

**é€‰æ‹© PyTorchï¼Œå¦‚æœä½ **ï¼š
- ğŸš€ éœ€è¦å¿«é€Ÿå¼€å‘å’Œè¿­ä»£
- ğŸ¢ åœ¨å·¥ä¸šç•Œéƒ¨ç½²æ¨¡å‹
- ğŸ‘¥ å›¢é˜Ÿå·²ç†Ÿæ‚‰ PyTorch
- ğŸ“š éœ€è¦ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹
- ğŸ”§ æ¨¡å‹åŒ…å«å¤æ‚æ§åˆ¶æµ

**é€‰æ‹© JAXï¼Œå¦‚æœä½ **ï¼š
- âš¡ è¿½æ±‚æè‡´æ€§èƒ½
- ğŸ”¬ ä»äº‹ç§‘å­¦è®¡ç®—ç ”ç©¶
- ğŸ§® éœ€è¦çµæ´»çš„è‡ªåŠ¨å¾®åˆ†
- â˜ï¸ ä½¿ç”¨ Google Cloud TPU
- ğŸ¯ åå¥½å‡½æ•°å¼ç¼–ç¨‹

### æ··åˆä½¿ç”¨

ä¸¤è€…å¹¶éäº’æ–¥ï¼Œå¯ä»¥ç»“åˆä½¿ç”¨ï¼š

1. **åŸå‹ â†’ ç”Ÿäº§**ï¼šPyTorch å¼€å‘ï¼ŒJAX ä¼˜åŒ–æ€§èƒ½å…³é”®éƒ¨åˆ†
2. **ç ”ç©¶ â†’ åº”ç”¨**ï¼šJAX ç ”ç©¶æ–°ç®—æ³•ï¼ŒPyTorch å·¥ç¨‹åŒ–
3. **äº’æ“ä½œ**ï¼šé€šè¿‡ ONNX æˆ– `jax2torch` è½¬æ¢æ¨¡å‹

### æœªæ¥è¶‹åŠ¿

**PyTorch**ï¼š
- `torch.compile`ï¼ˆPyTorch 2.0ï¼‰ç¼©å°æ€§èƒ½å·®è·
- æ›´å¥½çš„ TPU æ”¯æŒ
- æŒç»­ä¼˜åŒ–æ˜“ç”¨æ€§

**JAX**ï¼š
- ç”Ÿæ€ç³»ç»Ÿå¿«é€Ÿæˆé•¿
- æ›´å¤šé«˜çº§åº“ï¼ˆFlaxã€Equinoxï¼‰
- å·¥ä¸šç•Œé‡‡ç”¨å¢åŠ ï¼ˆDeepMindã€Googleï¼‰

## å‚è€ƒèµ„æº

### PyTorch
- [å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [PyTorch Lightning](https://lightning.ai/)

### JAX
- [å®˜æ–¹æ–‡æ¡£](https://jax.readthedocs.io/)
- [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html)
- [Flax Documentation](https://flax.readthedocs.io/)
- [Awesome JAX](https://github.com/n2cholas/awesome-jax)

---

**ä½œè€…**: zijian
**æ—¥æœŸ**: 2026-01-19
