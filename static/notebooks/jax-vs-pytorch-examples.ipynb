{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX vs PyTorch 代码示例\n",
    "\n",
    "本 Notebook 展示 JAX 和 PyTorch 的核心特性对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 JAX\n",
    "!pip install -q jax jaxlib\n",
    "\n",
    "# PyTorch 通常已预装在 Colab 中\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基础操作对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "x_torch = torch.randn(1000, 1000)\n",
    "y_torch = torch.randn(1000, 1000)\n",
    "\n",
    "start = time.time()\n",
    "z_torch = torch.matmul(x_torch, y_torch)\n",
    "print(f\"PyTorch 矩阵乘法: {time.time() - start:.4f}s\")\n",
    "\n",
    "# JAX\n",
    "x_jax = jax.random.normal(jax.random.PRNGKey(0), (1000, 1000))\n",
    "y_jax = jax.random.normal(jax.random.PRNGKey(1), (1000, 1000))\n",
    "\n",
    "start = time.time()\n",
    "z_jax = jnp.matmul(x_jax, y_jax)\n",
    "z_jax.block_until_ready()  # JAX 是异步的\n",
    "print(f\"JAX 矩阵乘法: {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自动微分对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 自动微分\n",
    "def pytorch_loss(x):\n",
    "    return (x ** 2).sum()\n",
    "\n",
    "x_torch = torch.randn(100, requires_grad=True)\n",
    "loss = pytorch_loss(x_torch)\n",
    "loss.backward()\n",
    "print(f\"PyTorch 梯度形状: {x_torch.grad.shape}\")\n",
    "print(f\"PyTorch 梯度前5个值: {x_torch.grad[:5]}\")\n",
    "\n",
    "# JAX 自动微分\n",
    "def jax_loss(x):\n",
    "    return (x ** 2).sum()\n",
    "\n",
    "x_jax = jax.random.normal(jax.random.PRNGKey(0), (100,))\n",
    "grad_fn = jax.grad(jax_loss)\n",
    "grads = grad_fn(x_jax)\n",
    "print(f\"\\nJAX 梯度形状: {grads.shape}\")\n",
    "print(f\"JAX 梯度前5个值: {grads[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. JIT 编译对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个复杂函数\n",
    "def complex_fn(x):\n",
    "    for _ in range(10):\n",
    "        x = x @ x.T\n",
    "        x = x + 1\n",
    "    return x.sum()\n",
    "\n",
    "# PyTorch (Eager)\n",
    "x_torch = torch.randn(100, 100)\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = complex_fn(x_torch)\n",
    "pytorch_time = time.time() - start\n",
    "print(f\"PyTorch (Eager): {pytorch_time:.4f}s\")\n",
    "\n",
    "# PyTorch (torch.compile)\n",
    "complex_fn_compiled = torch.compile(complex_fn)\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = complex_fn_compiled(x_torch)\n",
    "pytorch_compile_time = time.time() - start\n",
    "print(f\"PyTorch (Compiled): {pytorch_compile_time:.4f}s\")\n",
    "\n",
    "# JAX (JIT)\n",
    "def jax_complex_fn(x):\n",
    "    for _ in range(10):\n",
    "        x = x @ x.T\n",
    "        x = x + 1\n",
    "    return x.sum()\n",
    "\n",
    "jax_complex_fn_jit = jax.jit(jax_complex_fn)\n",
    "x_jax = jax.random.normal(jax.random.PRNGKey(0), (100, 100))\n",
    "\n",
    "# 预热\n",
    "_ = jax_complex_fn_jit(x_jax).block_until_ready()\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = jax_complex_fn_jit(x_jax).block_until_ready()\n",
    "jax_time = time.time() - start\n",
    "print(f\"JAX (JIT): {jax_time:.4f}s\")\n",
    "\n",
    "print(f\"\\n加速比:\")\n",
    "print(f\"PyTorch Compile vs Eager: {pytorch_time/pytorch_compile_time:.2f}x\")\n",
    "print(f\"JAX JIT vs PyTorch Eager: {pytorch_time/jax_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. vmap (自动向量化) - JAX 独有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单样本函数\n",
    "def predict_single(params, x):\n",
    "    W, b = params\n",
    "    return jnp.dot(x, W) + b\n",
    "\n",
    "# 手动批处理 (PyTorch 风格)\n",
    "def predict_batch_manual(params, X):\n",
    "    return jnp.stack([predict_single(params, x) for x in X])\n",
    "\n",
    "# 自动向量化 (JAX vmap)\n",
    "predict_batch_vmap = jax.vmap(predict_single, in_axes=(None, 0))\n",
    "\n",
    "# 测试\n",
    "W = jax.random.normal(jax.random.PRNGKey(0), (10, 5))\n",
    "b = jax.random.normal(jax.random.PRNGKey(1), (5,))\n",
    "params = (W, b)\n",
    "X = jax.random.normal(jax.random.PRNGKey(2), (32, 10))\n",
    "\n",
    "# 手动批处理\n",
    "start = time.time()\n",
    "result_manual = predict_batch_manual(params, X)\n",
    "manual_time = time.time() - start\n",
    "\n",
    "# vmap\n",
    "start = time.time()\n",
    "result_vmap = predict_batch_vmap(params, X)\n",
    "vmap_time = time.time() - start\n",
    "\n",
    "print(f\"手动批处理: {manual_time:.6f}s\")\n",
    "print(f\"vmap: {vmap_time:.6f}s\")\n",
    "print(f\"加速比: {manual_time/vmap_time:.2f}x\")\n",
    "print(f\"结果一致: {jnp.allclose(result_manual, result_vmap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 简单神经网络训练对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 版本\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model_torch = PyTorchMLP()\n",
    "optimizer = optim.Adam(model_torch.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 训练数据\n",
    "X_train = torch.randn(100, 10)\n",
    "y_train = torch.randn(100, 1)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model_torch(X_train)\n",
    "    loss = criterion(pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"PyTorch Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX 版本 (纯函数式)\n",
    "import optax\n",
    "\n",
    "def jax_mlp(params, x):\n",
    "    W1, b1, W2, b2 = params\n",
    "    x = jax.nn.relu(jnp.dot(x, W1) + b1)\n",
    "    return jnp.dot(x, W2) + b2\n",
    "\n",
    "def mse_loss(params, X, y):\n",
    "    pred = jax.vmap(lambda x: jax_mlp(params, x))(X)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "# 初始化参数\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key, 4)\n",
    "W1 = jax.random.normal(keys[0], (10, 64)) * 0.1\n",
    "b1 = jnp.zeros(64)\n",
    "W2 = jax.random.normal(keys[1], (64, 1)) * 0.1\n",
    "b2 = jnp.zeros(1)\n",
    "params = (W1, b1, W2, b2)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optax.adam(0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# 训练数据\n",
    "X_train_jax = jax.random.normal(keys[2], (100, 10))\n",
    "y_train_jax = jax.random.normal(keys[3], (100, 1))\n",
    "\n",
    "# JIT 编译训练步骤\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, X, y):\n",
    "    loss, grads = jax.value_and_grad(mse_loss)(params, X, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    params, opt_state, loss = train_step(params, opt_state, X_train_jax, y_train_jax)\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"JAX Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "通过以上示例可以看到：\n",
    "\n",
    "**PyTorch 优势**：\n",
    "- 代码更直观，易于理解\n",
    "- 面向对象设计，符合传统编程习惯\n",
    "- 调试方便\n",
    "\n",
    "**JAX 优势**：\n",
    "- JIT 编译带来显著性能提升\n",
    "- vmap 自动向量化非常强大\n",
    "- 函数式编程，可组合性强\n",
    "- 更灵活的自动微分\n",
    "\n",
    "选择哪个框架取决于你的具体需求！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
